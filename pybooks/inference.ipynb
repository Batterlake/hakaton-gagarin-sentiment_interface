{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/worker/workspace/dialogue-summarization/.conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/worker/workspace/dialogue-summarization/.conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/home/worker/workspace/dialogue-summarization/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "\n",
    "def seed_everything(seed=10):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "m_name = \"cointegrated/rut5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(m_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tp\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/worker/workspace/hakatons/hakaton-gagarin-sentiment_interface/src\")\n",
    "from t5.utils import evaluate_metric, generate_answer_batched\n",
    "\n",
    "EntityScoreType = tp.Tuple[int, float]  # (entity_id, entity_score)\n",
    "MessageResultType = tp.List[EntityScoreType]  # list of entity scores,\n",
    "#    for example, [(entity_id, entity_score) for entity_id, entity_score in entities_found]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = torch.load('final_model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"–°–®–ê –≤–≤–æ–¥—è—Ç —Å–∞–Ω–∫—Ü–∏–∏ –ø—Ä–æ—Ç–∏–≤ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –±–∞–Ω–∫–æ–≤:  –ú–æ—Å–∫–æ–≤—Å–∫–∏–π –∫—Ä–µ–¥–∏—Ç–Ω—ã–π –±–∞–Ω–∫ –ë–∞–Ω–∫ ¬´–£—Ä–∞—Å–∏–±¬ª #USBN –ú–¢–° –ë–∞–Ω–∫ #MTSS –ë–∞–Ω–∫ ¬´–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥¬ª #BSPB –ë–∞–Ω–∫ ¬´–ó–µ–Ω–∏—Ç¬ª –õ–∞–Ω—Ç–∞ –ë–∞–Ω–∫ –ú–µ—Ç–∞–ª–ª–∏–Ω–≤–µ—Å—Ç–±–∞–Ω–∫ –ë–∞–Ω–∫ ¬´–ü—Ä–∏–º–æ—Ä—å–µ¬ª –°–î–ú-–ë–∞–Ω–∫ –£—Ä–∞–ª—å—Å–∫–∏–π –±–∞–Ω–∫ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ —Ä–∞–∑–≤–∏—Ç–∏—è –ë–∞–Ω–∫ \"\"–õ–µ–≤–æ–±–µ—Ä–µ–∂–Ω—ã–π\"\"  \\xa0\\xa0\\xa0\\xa0 ‚Äî Frank Media  ‚ö†Ô∏èüá∫üá∏üá∑üá∫#—Å–∞–Ω–∫—Ü–∏–∏ #—Ä–æ—Å—Å–∏—è  –ú–∏–Ω—Ñ–∏–Ω –°–®–ê –≤ –Ω–æ–≤–æ–º –ø–∞–∫–µ—Ç–µ —Å–∞–Ω–∫—Ü–∏–π –ø—Ä–æ—Ç–∏–≤ –†–§ –≤–≤–æ–¥–∏—Ç —Ä–µ—Å—Ç—Ä–∏–∫—Ü–∏–∏ –ø—Ä–æ—Ç–∏–≤ 22 —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –ª–∏—Ü –∏ 83 —é—Ä–ª–∏—Ü.  –ú–∏–Ω—Ñ–∏–Ω –°–®–ê –≤–≤–æ–¥–∏—Ç —Å–∞–Ω–∫—Ü–∏–∏ –ø—Ä–æ—Ç–∏–≤ –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–π –º–µ—Ç–∞–ª–ª—É—Ä–≥–∏—á–µ—Å–∫–æ–≥–æ —Å–µ–∫—Ç–æ—Ä–∞ –∏ –≥–æ—Ä–Ω–æ–π –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç–∏ –†–§  –ú–∏–Ω—Ñ–∏–Ω –°–®–ê —Ç–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∞–Ω–∫—Ü–∏–∏ –ø—Ä–æ—Ç–∏–≤ –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–π –í–ü–ö –†–§  \\xa0\\xa0\\xa0\\xa0\\xa0 ‚Äî –¢–ê–°–°  üí•üá∑üá∫#TCSG = +6%  –¢–∏–Ω—å–∫–æ—Ñ—Ñ —Ç–∞–∫–∂–µ –Ω–µ –≤–æ—à–µ–ª –≤ —Å–ø–∏—Å–æ–∫ –Ω–æ–≤—ã—Ö —Å–∞–Ω–∫—Ü–∏–π\\xa0 –°–®–ê  (–ë—Ä–∏—Ç–∞–Ω–∏—è —Ç–æ–∂–µ –Ω–µ –≤–Ω–µ—Å–ª–∞)  #–±–∞–Ω–∫–∏ #—Å–∞–Ω–∫—Ü–∏–∏ #—Ä–æ—Å—Å–∏—è\"\n",
    "\n",
    "df = pd.DataFrame({\"message\": [s]})\n",
    "\n",
    "df[\"prefix\"] = \"clsorg\"\n",
    "df = df.rename({\"message\": \"input_text\"}, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# sys.path.append(\"/home/worker/workspace/hakatons/hakaton-gagarin-sentiment_interface/src\")\n",
    "from t5.model import NERModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_batched(\n",
    "    trained_model: NERModel,\n",
    "    tokenizer: T5Tokenizer,\n",
    "    data: pd.DataFrame,\n",
    "    batch_size: int = 64,\n",
    "):\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for name, batch in tqdm(data.groupby(np.arange(len(data)) // batch_size)):\n",
    "            source_encoding = tokenizer(\n",
    "                (batch[\"prefix\"] + \": \" + batch[\"input_text\"]).tolist(),\n",
    "                max_length=396,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                add_special_tokens=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            generated_ids = trained_model.model.generate(\n",
    "                input_ids=source_encoding[\"input_ids\"].cuda(),\n",
    "                attention_mask=source_encoding[\"attention_mask\"].cuda(),\n",
    "                num_beams=3,\n",
    "                max_length=80,\n",
    "                repetition_penalty=1.0,\n",
    "                early_stopping=True,\n",
    "                use_cache=True,\n",
    "            ).cpu()\n",
    "\n",
    "            preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            predictions.append(preds)\n",
    "\n",
    "    return sum(predictions, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions = generate_answer_batched(\n",
    "    trained_model=model, tokenizer=tokenizer, data=df, batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_found = predictions\n",
    "\n",
    "results = []\n",
    "\n",
    "for row in entities_found:\n",
    "    for entity in row.split(\";\"):\n",
    "        t = []\n",
    "        tup = entity.split('-')\n",
    "        entity_id, entity_score = tup\n",
    "        t.append((entity_id, entity_score))\n",
    "    results.append(t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('225', '2')]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_texts(\n",
    "    messages: tp.Iterable[str], *args, **kwargs\n",
    ") -> tp.Iterable[MessageResultType]:\n",
    "    \"\"\"\n",
    "    Main function (see tests for more clarifications)\n",
    "    Args:\n",
    "        messages (tp.Iterable[str]): any iterable of strings (utf-8 encoded text messages)\n",
    "\n",
    "    Returns:\n",
    "        tp.Iterable[tp.Tuple[int, float]]: for any messages returns MessageResultType object\n",
    "    -------\n",
    "    Clarifications:\n",
    "    >>> assert all([len(m) < 10 ** 11 for m in messages]) # all messages are shorter than 2048 characters\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame({\"message\": messages})\n",
    "\n",
    "    entities_found = generate_answer_batched(\n",
    "        trained_model=model, tokenizer=tokenizer, data=df, batch_size=64\n",
    "    )\n",
    "\n",
    "    for row in entities_found:\n",
    "        for entity in row.split(\";\"):\n",
    "            t = []\n",
    "            tup = entity.split('-')\n",
    "            entity_id, entity_score = tup\n",
    "            t.append((entity_id, entity_score))\n",
    "        results.append(t)\n",
    "\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
