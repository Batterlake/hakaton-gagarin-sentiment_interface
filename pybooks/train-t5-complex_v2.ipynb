{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/worker/workspace/dialogue-summarization/.conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/worker/workspace/dialogue-summarization/.conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/home/worker/workspace/dialogue-summarization/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "\n",
    "def seed_everything(seed=10):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text: str):\n",
    "    def remove_hashtag_and_question_mark_sequences(input_string):\n",
    "        # Remove \"#\" followed by symbols\n",
    "        string_without_hashtags = re.sub(r'#\\w+', '', input_string)\n",
    "        # Remove sequences of \"?\"\n",
    "        return re.sub(r'\\?+', '', string_without_hashtags)\n",
    "    \n",
    "    def remove_emojis(input_string):\n",
    "    # Emoji ranges: 1F600 - 1F64F, 1F300 - 1F5FF, 1F680 - 1F6FF, 1F700 - 1F77F, 1F780 - 1F7FF, 1F800 - 1F8FF, 1F900 - 1F9FF, 1FA00 - 1FA6F, 1FA70 - 1FAFF, 2600 - 26FF, 2700 - 27BF, 20D0 - 20FF, FE0F, and a few others\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                                \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                                \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                                \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                                \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                                \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                                \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                                \"\\u2600-\\u26FF\\u2700-\\u27BF\"  # dingbats\n",
    "                                \"\\u20D0-\\u20FF\\uFE0F\"  # combining enclosing keycap & VARIATION SELECTOR-16\n",
    "                                \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', input_string)\n",
    "\n",
    "    result = remove_emojis(text)\n",
    "\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>prefix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5222</th>\n",
       "      <td>üá∑üá∫#TCSG #–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å  –ö–û–ù–°–ï–ù–°–£–°: TCS Group –≤–æ I...</td>\n",
       "      <td>225-4</td>\n",
       "      <td>clsorg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4135</th>\n",
       "      <td>‚è∞ –î–æ–±—Ä–æ–µ —É—Ç—Ä–æ! 16 –º–∞—Ä—Ç–∞  üåç –ù–æ—á–Ω–æ–µ –¥–µ–∂—É—Ä—Å—Ç–≤–æ (–∑...</td>\n",
       "      <td>228-3;251-3</td>\n",
       "      <td>clsorg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3130</th>\n",
       "      <td>–°–ü–ë –ë–∏—Ä–∂–∞ –Ω–∞—á–Ω–µ—Ç —Ç–æ—Ä–≥–∏ —Ü–µ–Ω–Ω—ã–º–∏ –±—É–º–∞–≥–∞–º–∏ –≤–æ—Å—å–º–∏...</td>\n",
       "      <td>255-4</td>\n",
       "      <td>clsorg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>#SMLT  –û—Ç–∫—É–ø –≤ –∞–∫—Ü–∏—è—Ö –°–∞–º–æ–ª—ë—Ç–∞: –≥—ç–ø –Ω–∞ –æ—Ç–∫—Ä—ã—Ç–∏...</td>\n",
       "      <td>56-3</td>\n",
       "      <td>clsorg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3917</th>\n",
       "      <td>‚Äã‚Äãüü¢ –ò–¢–û–ì–ò –î–ù–Ø. –†–æ—Å—Å–∏–π—Å–∫–∏–µ –∞–∫—Ü–∏–∏ –Ω–µ–º–Ω–æ–≥–æ –ø–æ–¥—Ä–æ—Å...</td>\n",
       "      <td>90-2;152-2</td>\n",
       "      <td>clsorg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4392</th>\n",
       "      <td>‚õîÔ∏è –†–æ—Å—Å–∏—è –∑–∞–ø—Ä–µ—â–∞–µ—Ç —ç–∫—Å–ø–æ—Ä—Ç –±–µ–Ω–∑–∏–Ω–∞. –ö–∞–∫–∏–µ –∫–æ–º...</td>\n",
       "      <td>25-2</td>\n",
       "      <td>clsorg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5193</th>\n",
       "      <td>üá∑üá∫#SPBE #–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å  –ò—Ç–æ–≥–∏ —Ç–æ—Ä–≥–æ–≤ –Ω–∞ –°–ü–ë –ë–∏—Ä–∂–µ...</td>\n",
       "      <td>255-3</td>\n",
       "      <td>clsorg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>–ü—Ä–æ—Å—Ç–æ –≤—Å–ø–æ–º–Ω–∏—Ç–µ –∫–∞–∫ –ø—Ä–æ—Å—Ä–∞–ª—Å—è –°–µ–≤–∫–∞ –Ω–∞ –æ–∂–∏–¥–∞–Ω...</td>\n",
       "      <td>90-5</td>\n",
       "      <td>clsorg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5391</th>\n",
       "      <td>üá∑üá∫#–∞–≤–∏–∞ #—Ä–æ—Å—Å–∏—è  –†–æ—Å—Å–∏—è –º–æ–∂–µ—Ç –æ—Ç–∫—Ä—ã—Ç—å –ø—Ä—è–º—ã–µ –∞...</td>\n",
       "      <td>32-4</td>\n",
       "      <td>clsorg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4306</th>\n",
       "      <td>‚ö°Ô∏è –°–±–µ—Ä (SBER) –æ—Ç—ã–≥—Ä–∞–ª –≤—Å—ë –ø–∞–¥–µ–Ω–∏–µ –Ω–∞ –°–í–û. #—Ö–≤...</td>\n",
       "      <td>150-3</td>\n",
       "      <td>clsorg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>#MSNG #–û—Ç—á–µ—Ç–Ω–æ—Å—Ç—å  –ß–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å –ú–æ—Å—ç–Ω–µ—Ä–≥–æ –ø–æ...</td>\n",
       "      <td>215-4</td>\n",
       "      <td>clsorg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5347</th>\n",
       "      <td>üá∑üá∫#YNDX  –ì—Ä—É–ø–ø–∞ –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤, –≤ –∫–æ—Ç–æ—Ä—É—é –º–æ–≥—É—Ç –≤–æ...</td>\n",
       "      <td>236-3</td>\n",
       "      <td>clsorg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4193</th>\n",
       "      <td>‚ö†Ô∏èüá∑üá∫#CHMF #—Å–∞–Ω–∫—Ü–∏–∏  –ï–≤—Ä–æ–ø–µ–π—Å–∫–∏–π —Å—É–¥ –æ—Ç–∫–∞–∑–∞–ª—Å—è ...</td>\n",
       "      <td>152-2</td>\n",
       "      <td>clsorg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6753</th>\n",
       "      <td>üõ¢&gt;87 –¥–æ–ª–ª./–±–∞—Ä—Ä.  —Å—Ç–æ–∏—Ç Brent –≤–ø–µ—Ä–≤—ã–µ —Å —Å–µ—Ä–µ–¥–∏...</td>\n",
       "      <td>111-4</td>\n",
       "      <td>clsorg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4171</th>\n",
       "      <td>‚ö†Ô∏è#–∞–ª–º–∞–∑—ã #–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å #ALRS  De Beers –∑–∞ –ø–æ—Å–ª–µ...</td>\n",
       "      <td>4-2</td>\n",
       "      <td>clsorg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3849</th>\n",
       "      <td>‚Äã‚Äãüë©‚Äçüëß –ú–∞—Ç—å –∏ –î–∏—Ç—è (MDMG) - –æ–±–∑–æ—Ä —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä–µ...</td>\n",
       "      <td>224-2</td>\n",
       "      <td>clsorg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2986</th>\n",
       "      <td>–ü—Ä–æ–¥–∞–∂–∏ —É—Å–∏–ª–∏–≤–∞—é—Ç—Å—è —Å –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ–º –∫–ª—é—á–µ–≤–æ–≥–æ —É...</td>\n",
       "      <td>36-3</td>\n",
       "      <td>clsorg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>‚ùóÔ∏èüá∑üá∫#ISKJ #event —Å–µ–≥–æ–¥–Ω—è - 18:00–º—Å–∫ –í–µ–±–∏–Ω–∞—Ä Sb...</td>\n",
       "      <td>150-3</td>\n",
       "      <td>clsorg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656</th>\n",
       "      <td>#SOFL –°–æ—Ñ—Ç–ª–∞–π–Ω –≤–µ–¥–µ—Ç –ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã –æ –ø–æ–∫—É–ø–∫–µ 100%...</td>\n",
       "      <td>237-4</td>\n",
       "      <td>clsorg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>\"üá∑üá∫#IRAO  –°—É–¥ –≤ –õ–∏—Ç–≤–µ –ø–æ—Å—Ç–∞–Ω–æ–≤–∏–ª –≤—ã–≤–µ—Å—Ç–∏ –∏–∑ —Å–∞...</td>\n",
       "      <td>72-4</td>\n",
       "      <td>clsorg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             input_text  target_text  prefix\n",
       "5222  üá∑üá∫#TCSG #–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å  –ö–û–ù–°–ï–ù–°–£–°: TCS Group –≤–æ I...        225-4  clsorg\n",
       "4135  ‚è∞ –î–æ–±—Ä–æ–µ —É—Ç—Ä–æ! 16 –º–∞—Ä—Ç–∞  üåç –ù–æ—á–Ω–æ–µ –¥–µ–∂—É—Ä—Å—Ç–≤–æ (–∑...  228-3;251-3  clsorg\n",
       "3130  –°–ü–ë –ë–∏—Ä–∂–∞ –Ω–∞—á–Ω–µ—Ç —Ç–æ—Ä–≥–∏ —Ü–µ–Ω–Ω—ã–º–∏ –±—É–º–∞–≥–∞–º–∏ –≤–æ—Å—å–º–∏...        255-4  clsorg\n",
       "1629  #SMLT  –û—Ç–∫—É–ø –≤ –∞–∫—Ü–∏—è—Ö –°–∞–º–æ–ª—ë—Ç–∞: –≥—ç–ø –Ω–∞ –æ—Ç–∫—Ä—ã—Ç–∏...         56-3  clsorg\n",
       "3917  ‚Äã‚Äãüü¢ –ò–¢–û–ì–ò –î–ù–Ø. –†–æ—Å—Å–∏–π—Å–∫–∏–µ –∞–∫—Ü–∏–∏ –Ω–µ–º–Ω–æ–≥–æ –ø–æ–¥—Ä–æ—Å...   90-2;152-2  clsorg\n",
       "4392  ‚õîÔ∏è –†–æ—Å—Å–∏—è –∑–∞–ø—Ä–µ—â–∞–µ—Ç —ç–∫—Å–ø–æ—Ä—Ç –±–µ–Ω–∑–∏–Ω–∞. –ö–∞–∫–∏–µ –∫–æ–º...         25-2  clsorg\n",
       "5193  üá∑üá∫#SPBE #–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å  –ò—Ç–æ–≥–∏ —Ç–æ—Ä–≥–æ–≤ –Ω–∞ –°–ü–ë –ë–∏—Ä–∂–µ...        255-3  clsorg\n",
       "2996  –ü—Ä–æ—Å—Ç–æ –≤—Å–ø–æ–º–Ω–∏—Ç–µ –∫–∞–∫ –ø—Ä–æ—Å—Ä–∞–ª—Å—è –°–µ–≤–∫–∞ –Ω–∞ –æ–∂–∏–¥–∞–Ω...         90-5  clsorg\n",
       "5391  üá∑üá∫#–∞–≤–∏–∞ #—Ä–æ—Å—Å–∏—è  –†–æ—Å—Å–∏—è –º–æ–∂–µ—Ç –æ—Ç–∫—Ä—ã—Ç—å –ø—Ä—è–º—ã–µ –∞...         32-4  clsorg\n",
       "4306  ‚ö°Ô∏è –°–±–µ—Ä (SBER) –æ—Ç—ã–≥—Ä–∞–ª –≤—Å—ë –ø–∞–¥–µ–Ω–∏–µ –Ω–∞ –°–í–û. #—Ö–≤...        150-3  clsorg\n",
       "1492  #MSNG #–û—Ç—á–µ—Ç–Ω–æ—Å—Ç—å  –ß–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å –ú–æ—Å—ç–Ω–µ—Ä–≥–æ –ø–æ...        215-4  clsorg\n",
       "5347  üá∑üá∫#YNDX  –ì—Ä—É–ø–ø–∞ –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤, –≤ –∫–æ—Ç–æ—Ä—É—é –º–æ–≥—É—Ç –≤–æ...        236-3  clsorg\n",
       "4193  ‚ö†Ô∏èüá∑üá∫#CHMF #—Å–∞–Ω–∫—Ü–∏–∏  –ï–≤—Ä–æ–ø–µ–π—Å–∫–∏–π —Å—É–¥ –æ—Ç–∫–∞–∑–∞–ª—Å—è ...        152-2  clsorg\n",
       "6753  üõ¢>87 –¥–æ–ª–ª./–±–∞—Ä—Ä.  —Å—Ç–æ–∏—Ç Brent –≤–ø–µ—Ä–≤—ã–µ —Å —Å–µ—Ä–µ–¥–∏...        111-4  clsorg\n",
       "4171  ‚ö†Ô∏è#–∞–ª–º–∞–∑—ã #–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å #ALRS  De Beers –∑–∞ –ø–æ—Å–ª–µ...          4-2  clsorg\n",
       "3849  ‚Äã‚Äãüë©‚Äçüëß –ú–∞—Ç—å –∏ –î–∏—Ç—è (MDMG) - –æ–±–∑–æ—Ä —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä–µ...        224-2  clsorg\n",
       "2986  –ü—Ä–æ–¥–∞–∂–∏ —É—Å–∏–ª–∏–≤–∞—é—Ç—Å—è —Å –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ–º –∫–ª—é—á–µ–≤–æ–≥–æ —É...         36-3  clsorg\n",
       "4497  ‚ùóÔ∏èüá∑üá∫#ISKJ #event —Å–µ–≥–æ–¥–Ω—è - 18:00–º—Å–∫ –í–µ–±–∏–Ω–∞—Ä Sb...        150-3  clsorg\n",
       "1656  #SOFL –°–æ—Ñ—Ç–ª–∞–π–Ω –≤–µ–¥–µ—Ç –ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã –æ –ø–æ–∫—É–ø–∫–µ 100%...        237-4  clsorg\n",
       "692   \"üá∑üá∫#IRAO  –°—É–¥ –≤ –õ–∏—Ç–≤–µ –ø–æ—Å—Ç–∞–Ω–æ–≤–∏–ª –≤—ã–≤–µ—Å—Ç–∏ –∏–∑ —Å–∞...         72-4  clsorg"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [pin]\n",
    "file_path = \"../data/data-hard.csv\"\n",
    "root_path = \"../data/\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df[\"prefix\"] = \"clsorg\"\n",
    "df = df.rename({\"message\": \"input_text\", \"label\": \"target_text\"}, axis=1)\n",
    "\n",
    "# df[\"input_text\"] = df[\"input_text\"].apply(preprocess_text)\n",
    "df.drop(2764)\n",
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "m_name = \"cointegrated/rut5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(m_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitelist = [str(num) for num in range(280)]\n",
    "whitelist_ids = [tokenizer.encode(word)[0] for word in whitelist]\n",
    "bad_words_ids=[[id] for id in range(tokenizer.vocab_size) if id not in whitelist_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/worker/workspace/hakatons/hakaton-gagarin-sentiment_interface/src\")\n",
    "\n",
    "from t5.dataset import NERDataModel\n",
    "from t5.model import NERModel\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 9\n",
    "train_df, test_df = train_test_split(df, test_size=0.25, random_state=42)\n",
    "data_module = NERDataModel(train_df, test_df, tokenizer, batch_size=BATCH_SIZE)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(20100, 512)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(20100, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 6)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-7): 7 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(20100, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 6)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-7): 7 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=20100, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/worker/workspace/dialogue-summarization/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "model = NERModel(m_name, lr=0.0007)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints2\",\n",
    "    filename=\"ner\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    callbacks=[checkpoint_callback],\n",
    "    max_epochs=EPOCHS,\n",
    "    accelerator=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: /home/worker/workspace/hakatons/hakaton-gagarin-sentiment_interface/pybooks/lightning_logs\n",
      "/home/worker/workspace/dialogue-summarization/.conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:639: Checkpoint directory /home/worker/workspace/hakatons/hakaton-gagarin-sentiment_interface/pybooks/checkpoints2 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [6]\n",
      "/home/worker/workspace/dialogue-summarization/.conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 64.6 M\n",
      "-----------------------------------------------------\n",
      "64.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "64.6 M    Total params\n",
      "258.578   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/worker/workspace/dialogue-summarization/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (43) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:44<00:00,  0.97it/s, v_num=0, train_loss=1.820, val_loss=1.780]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 43: 'val_loss' reached 1.78439 (best 1.78439), saving model to '/home/worker/workspace/hakatons/hakaton-gagarin-sentiment_interface/pybooks/checkpoints2/ner-v15.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:44<00:00,  0.96it/s, v_num=0, train_loss=1.500, val_loss=1.430]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 86: 'val_loss' reached 1.42756 (best 1.42756), saving model to '/home/worker/workspace/hakatons/hakaton-gagarin-sentiment_interface/pybooks/checkpoints2/ner-v15.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:44<00:00,  0.96it/s, v_num=0, train_loss=1.270, val_loss=1.080]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 129: 'val_loss' reached 1.08011 (best 1.08011), saving model to '/home/worker/workspace/hakatons/hakaton-gagarin-sentiment_interface/pybooks/checkpoints2/ner-v15.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:21<00:18,  1.08it/s, v_num=0, train_loss=1.110, val_loss=1.080]"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# checkpoint_path = \"best_model.pth\"\n",
    "\n",
    "# # Save the entire model state dictionary\n",
    "# torch.save(model.state_dict(), checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# trained_model = NERModel(m_name)\n",
    "# trained_model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_model = NERModel.load_from_checkpoint(\"/home/worker/workspace/hakatons/hakaton-gagarin-sentiment_interface/pybooks/checkpoints2/ner-v2.ckpt\")\n",
    "# trained_model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NERModel(\n",
       "  (model): T5ForConditionalGeneration(\n",
       "    (shared): Embedding(20100, 512)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(20100, 512)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 6)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-7): 7 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(20100, 512)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 6)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-7): 7 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=512, out_features=20100, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:14<00:00,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "from t5.utils import evaluate_metric, generate_answer_batched\n",
    "\n",
    "predictions = generate_answer_batched(\n",
    "    trained_model=model, tokenizer=tokenizer, data=test_df, batch_size=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf = test_df.copy()\n",
    "ldf[\"predictions\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>prefix</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3337</th>\n",
       "      <td>–¢—Ä–µ–Ω–¥—ã, —Ü–∏—Ñ—Ä—ã, —Ñ–∞–∫—Ç—ã: —Ä—ã–Ω–æ–∫ 14 –º–∞—Ä—Ç–∞   üìâ –í —Å—Ä–µ...</td>\n",
       "      <td>111-4;241-3;160-4</td>\n",
       "      <td>clsorg</td>\n",
       "      <td>235-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5144</th>\n",
       "      <td>üá∑üá∫#SBER #–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å  –°–ë–ï–†–ë–ê–ù–ö –í –Ø–ù–í–ê–†–ï 2023–ì –£...</td>\n",
       "      <td>150-5</td>\n",
       "      <td>clsorg</td>\n",
       "      <td>150-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5062</th>\n",
       "      <td>üá∑üá∫#POSI #–¥–∏–≤–∏–¥–µ–Ω–¥ –°–æ–≤–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ Positive Te...</td>\n",
       "      <td>241-4</td>\n",
       "      <td>clsorg</td>\n",
       "      <td>127-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4417</th>\n",
       "      <td>‚ú¥Ô∏è#MATIC #–∫—Ä–∏–ø—Ç–æ  Polygon —Å—Ç–∞–ª —Å–≤–æ–µ–≥–æ —Ä–æ–¥–∞ –º–∞—è...</td>\n",
       "      <td>235-0</td>\n",
       "      <td>clsorg</td>\n",
       "      <td>235-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>#SBER –°–±–µ—Ä–±–∞–Ω–∫ üî∑ –ê–Ω–æ–º–∞–ª—å–Ω—ã–π –æ–±—ä—ë–º –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ...</td>\n",
       "      <td>150-5</td>\n",
       "      <td>clsorg</td>\n",
       "      <td>236-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5871</th>\n",
       "      <td>üí•üá∑üá∫#GMKN = –º–∞–∫—Å –∑–∞ 1  –º–µ—Å</td>\n",
       "      <td>53-3</td>\n",
       "      <td>clsorg</td>\n",
       "      <td>227-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4325</th>\n",
       "      <td>‚ö°Ô∏è‚ö°Ô∏è‚ö°Ô∏è –ü–æ –∏—Ç–æ–≥–∞–º 2023 –≥–æ–¥–∞ –¥–∏–≤–∏–¥–µ–Ω–¥ –ú–ú–ö (MAGN)...</td>\n",
       "      <td>90-4</td>\n",
       "      <td>clsorg</td>\n",
       "      <td>221-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>\"üèÅ –ò—Ç–æ–≥–∏ –¥–Ω—è: 4 –∞–ø—Ä–µ–ª—è  üìà –ü—Ä–∏–≤–∏–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞–∫...</td>\n",
       "      <td>36-4;160-4;187-3;48-3</td>\n",
       "      <td>clsorg</td>\n",
       "      <td>111-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7032</th>\n",
       "      <td>üü¢ –ù–æ–≤—ã–π –≤—ã–ø—É—Å–∫ –µ–∂–µ–Ω–µ–¥–µ–ª—å–Ω–æ–≥–æ –≤–∏–¥–µ–æ Top News –æ—Ç...</td>\n",
       "      <td>32-3;227-3;111-3;90-3</td>\n",
       "      <td>clsorg</td>\n",
       "      <td>116-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2312</th>\n",
       "      <td>–ì–ª–∞–≤–Ω–æ–µ –∑–∞ –Ω–µ–¥–µ–ª—é.  –î–∏–≤–∏–¥–µ–Ω–¥–Ω—ã–π —Å–µ–∑–æ–Ω  –ù–∞—á–∏–Ω–∞–µ...</td>\n",
       "      <td>36-3</td>\n",
       "      <td>clsorg</td>\n",
       "      <td>23-3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1797 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             input_text  \\\n",
       "3337  –¢—Ä–µ–Ω–¥—ã, —Ü–∏—Ñ—Ä—ã, —Ñ–∞–∫—Ç—ã: —Ä—ã–Ω–æ–∫ 14 –º–∞—Ä—Ç–∞   üìâ –í —Å—Ä–µ...   \n",
       "5144  üá∑üá∫#SBER #–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å  –°–ë–ï–†–ë–ê–ù–ö –í –Ø–ù–í–ê–†–ï 2023–ì –£...   \n",
       "5062  üá∑üá∫#POSI #–¥–∏–≤–∏–¥–µ–Ω–¥ –°–æ–≤–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ Positive Te...   \n",
       "4417  ‚ú¥Ô∏è#MATIC #–∫—Ä–∏–ø—Ç–æ  Polygon —Å—Ç–∞–ª —Å–≤–æ–µ–≥–æ —Ä–æ–¥–∞ –º–∞—è...   \n",
       "1615  #SBER –°–±–µ—Ä–±–∞–Ω–∫ üî∑ –ê–Ω–æ–º–∞–ª—å–Ω—ã–π –æ–±—ä—ë–º –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ...   \n",
       "...                                                 ...   \n",
       "5871                          üí•üá∑üá∫#GMKN = –º–∞–∫—Å –∑–∞ 1  –º–µ—Å   \n",
       "4325  ‚ö°Ô∏è‚ö°Ô∏è‚ö°Ô∏è –ü–æ –∏—Ç–æ–≥–∞–º 2023 –≥–æ–¥–∞ –¥–∏–≤–∏–¥–µ–Ω–¥ –ú–ú–ö (MAGN)...   \n",
       "1027  \"üèÅ –ò—Ç–æ–≥–∏ –¥–Ω—è: 4 –∞–ø—Ä–µ–ª—è  üìà –ü—Ä–∏–≤–∏–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞–∫...   \n",
       "7032  üü¢ –ù–æ–≤—ã–π –≤—ã–ø—É—Å–∫ –µ–∂–µ–Ω–µ–¥–µ–ª—å–Ω–æ–≥–æ –≤–∏–¥–µ–æ Top News –æ—Ç...   \n",
       "2312  –ì–ª–∞–≤–Ω–æ–µ –∑–∞ –Ω–µ–¥–µ–ª—é.  –î–∏–≤–∏–¥–µ–Ω–¥–Ω—ã–π —Å–µ–∑–æ–Ω  –ù–∞—á–∏–Ω–∞–µ...   \n",
       "\n",
       "                target_text  prefix predictions  \n",
       "3337      111-4;241-3;160-4  clsorg       235-4  \n",
       "5144                  150-5  clsorg       150-3  \n",
       "5062                  241-4  clsorg       127-4  \n",
       "4417                  235-0  clsorg       235-4  \n",
       "1615                  150-5  clsorg       236-3  \n",
       "...                     ...     ...         ...  \n",
       "5871                   53-3  clsorg       227-3  \n",
       "4325                   90-4  clsorg       221-4  \n",
       "1027  36-4;160-4;187-3;48-3  clsorg       111-4  \n",
       "7032  32-3;227-3;111-3;90-3  clsorg       116-4  \n",
       "2312                   36-3  clsorg        23-3  \n",
       "\n",
       "[1797 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ldf[[\"tcomp\", \"tsent\"]] = (\n",
    "    ldf[\"target_text\"].str.split(\";\", expand=True)[0].str.split(\"-\", expand=True)\n",
    ")\n",
    "ldf[[\"pcomp\", \"psent\"]] = (\n",
    "    ldf[\"predictions\"].str.split(\";\", expand=True)[0].str.split(\"-\", expand=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for index, row in ldf.iterrows():\n",
    "    pcomp_digits = re.sub(r'\\D', '', str(row['pcomp']))\n",
    "  \n",
    "    if pcomp_digits == '':\n",
    "        ldf.at[index, 'pcomp'] = '0' \n",
    "    else:\n",
    "        ldf.at[index, 'pcomp'] = pcomp_digits \n",
    "        # try:\n",
    "        #     i = int(row['pcomp'])\n",
    "        # except ValueError:\n",
    "        #     print(row)\n",
    "        #     ldf.at[index, 'pcomp'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': 66.44030859234566,\n",
       " 'f1': 0.7127794606616044,\n",
       " 'accuracy': 0.6160267111853088}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [pin]\n",
    "\n",
    "evaluate_metric(\n",
    "    company_predictions=ldf[\"pcomp\"].tolist(),\n",
    "    company_labels=ldf[\"tcomp\"].tolist(),\n",
    "    sentiment_predictions=ldf[\"psent\"].tolist(),\n",
    "    sentiment_labels=ldf[\"tsent\"].tolist(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
